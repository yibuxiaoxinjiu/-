- ### 前言

1.ChatGPT发布后，大模型成为热门关键词。

2.大模型成为发展通用人工智能的重要途径。
	专用模型
	      针对特定任务，一个模型解决一个问题
	通用大模型
		一个模型应对多种任务、多种模态

3.书生·浦语开源历程

4.书生·浦语大模型系列
	轻量级：InternLM-7B
		应用场景：社区demo
	中量级：InternLM-20B
		应用场景：可开发定制高精度小规模模型
	重量级：InternLM-123B
		应用场景：通用大模型，千亿级模型规模。

5.书生·浦语20B开源大模型性能
	全面领先相近参数的开源模型
	以不足三分之一的参数，达到Llama2-70B水平

- ### 从模型到应用

1.流程步骤
![[Pasted image 20240103213607.png]]

- ### 数据

1.文本数据：50亿个文档
2.图像-文本数据：超2200个文件
3.视频数据：超1000个文件
4.OpenDataLab：丰富多样的开放数据（可训练30+的模态）

- ### 预训练

1.**高可扩展**（可扩展多卡分布式训练）
2.性能优化
3.高兼容性（兼容多种算法）
4.开箱即用

- ### 微调

1.增量续训
2.有监督微调
3.高效微调框架：**XTuner**

- ### 部署

1.**LMDeploy**解决大模型在GPU上部署的全流程解决方案

- ### 评测

1.**OpenCompass**开源评测架构

- ### 应用

1.轻量级智能体框架：**Lagent**
2.多模态智能体工具箱：**AgentLego**
